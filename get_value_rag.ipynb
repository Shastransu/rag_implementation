{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6463e3cd",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a93011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.globals import set_verbose, set_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3f9a8",
   "metadata": {},
   "source": [
    "## Set the required variables and file imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778acaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f709df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable verbose logging\n",
    "set_verbose(False)\n",
    "\n",
    "# Disable debug logging\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09fe3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI API Key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d3e06",
   "metadata": {},
   "source": [
    "## Setup the model and Langchain template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9d5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8862d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "## TASK\n",
    "Answer the question based only on the context provided below\n",
    "\n",
    "## CONTEXT\n",
    "{context}\n",
    "\n",
    "## QUESTION \n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Langchain prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316ddac",
   "metadata": {},
   "source": [
    "## Setting up the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739e748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Some question to be asked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cfbcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702c579",
   "metadata": {},
   "source": [
    "## PineCone setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"<put the index name which was given>\"\n",
    "namespace = \"<put the namespace name which was given>\"\n",
    "\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38512b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.80\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    print(\"User:\", queries[i])\n",
    "    response = vectorstore.similarity_search_with_score(queries[i], k=10)\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for j in range(len(response)):\n",
    "        if response[j][1]>threshold:\n",
    "            context += response[j][0].page_content + \"\\n\\n\"\n",
    "    \n",
    "    if context==\"\":\n",
    "        print(\"Sorry, I cannot help you with that question.\")\n",
    "        print(\"\\n\")\n",
    "        print(f'----------------------------------End of Question {i+1}------------------------------------')\n",
    "        print(\"\\n\")\n",
    "        continue\n",
    "    \n",
    "    prompt_formatted_str: str = prompt.format(context=context, question=queries)\n",
    "    \n",
    "    answer = model.predict(prompt_formatted_str)\n",
    "    print(\"Agent Reply:\", answer)\n",
    "    print(\"\\n\")\n",
    "    print(\"--------------------------------------Context---------------------------------------------\")\n",
    "    for k in range(len(response)):\n",
    "        print(response[k])\n",
    "    print(f'----------------------------------End of Question {i+1}------------------------------------')\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
