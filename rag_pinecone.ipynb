{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca18850",
   "metadata": {},
   "source": [
    "# Uploading document data to PineconeDB as vector embeddings\n",
    "Use the parsed txt files and convert them into chunks with meaningful size. Create the vector embeddings on the chunks and upload them to Pinecone.\n",
    "\n",
    "## Steps involved:\n",
    "Step-1: Basic cleaning of the parsed documents (Using Python and manually)\n",
    "\n",
    "Step-2: Use [LangChain](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/character_text_splitter/) to create the text chunks for each heading/section\n",
    "\n",
    "Step-3: Initiate the [OpenAI embedding](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) model to create vector embeddings of the chunks created\n",
    "\n",
    "Step-4: Upload the embeddings to [Pinecone](https://www.pinecone.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46e11d",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a9e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.globals import set_verbose, set_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14604026",
   "metadata": {},
   "source": [
    "## Set the required variables and file imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa126793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load.env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876ecca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable verbose logging\n",
    "set_verbose(False)\n",
    "\n",
    "# Disable debug logging\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d41c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_directory(path):\n",
    "    \"\"\"Gets all files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        path: The directory path.\n",
    "\n",
    "    Returns:\n",
    "        A list of filenames in the directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [f for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)))]\n",
    "    # return [f for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)) and f[-3:]==\"txt\")]\n",
    "    # return [f for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)) and f==\"telecalm_KB_parsed.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9361ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "raw_file_dir = os.getcwd()\n",
    "\n",
    "\n",
    "input_parsed_file_dir = raw_file_dir + \"/parsed_docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb9938",
   "metadata": {},
   "source": [
    "## Document Reading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedcd3dd",
   "metadata": {},
   "source": [
    "### Reading parsed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336845ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the txt files in the given directory\n",
    "input_parsed_file_paths = get_files_in_directory(input_parsed_file_dir)\n",
    "\n",
    "print(f\"Number of parsed text documents in the directory: {len(input_parsed_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_parsed_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa53d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_parsed_file_paths)):\n",
    "    print(f\"Currently reading: {input_parsed_file_paths[i]}\")\n",
    "\n",
    "    f = open(input_parsed_file_dir + \"/\" + input_parsed_file_paths[i], \"r\")\n",
    "    file_content = f.read()\n",
    "    all_content += file_content + \"\\n\\n\"\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f12170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdbb1f",
   "metadata": {},
   "source": [
    "### document cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text based on markdown\n",
    "all_docs_sep = all_content.split(\"\\n# \")\n",
    "\n",
    "print(f\"Number of chunks/documents: {len(all_docs_sep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28ed222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the uneven newline spacing/characters to keep everything properly formatted\n",
    "for i in range(len(all_docs_sep)):\n",
    "    all_docs_sep[i] = \"# \" + all_docs_sep[i].replace(\"# \", \"\").replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "# Combine all the text to form single text which is to be passed for chunking\n",
    "all_docs_sep_final = \"\\n\".join(all_docs_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4415d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbl_split_content = len(all_docs_sep_final.split(\"\\n\\n\"))\n",
    "print(f\"Number of chunks/documents on splitting based of double newline: {dbl_split_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1b8bd",
   "metadata": {},
   "source": [
    "## Converting document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289cd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each question is split into a separate chunk based on the \"\\n\\n\" separator\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(all_docs_sep_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1471113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chunks to Document objects\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "print(f\"Number of documents in the Knowledge Base: {len(docs)}\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2178e70",
   "metadata": {},
   "source": [
    "## Embediing model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d7076d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Some query question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431afb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convert the query to OpenAI embedding format\n",
    "embedded_query = embeddings.embed_query(query)\n",
    "\n",
    "# Check the size and see the embedding\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8604e",
   "metadata": {},
   "source": [
    "## Uploading the Vectors to PineconeDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"index_name\"\n",
    "namespace = \"namespace_name\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    docs, embeddings, index_name=index_name, namespace=namespace\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
